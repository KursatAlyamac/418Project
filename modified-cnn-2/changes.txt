Key changes made:
Increased initial filters from 32 to 64 to capture more features
Added BatchNormalization layers after convolutions to stabilize training
Replaced AvgPool2d with MaxPool2d for better feature selection
Added an additional conv block with 16 filters for deeper feature extraction

These changes should help reduce loss while keeping reasonable training time because:
BatchNorm helps with faster convergence
The deeper network with more filters can capture more complex features
MaxPooling helps retain important features
The gradual reduction in filter size (64->32->16->1) maintains a good balance between feature extraction and computational efficiency